#!/bin/bash

# vars
domain=$1
dirs=./$domain
urldir=$dirs/urls
aquatonedir=$dirs/aquatone


# clrs
red=`tput setaf 1`
green=`tput setaf 2`
yellow=`tput setaf 3`
reset=`tput sgr0`

main()
{
        workspace
        subdomaingathering
        filtering_scanning
        urlgathering
}
workspace()
{
        mkdir -p $dirs $urldir
}

subdomaingathering()
{
        # Use CLI tools and API's to gather subdomains, and pipe it into subs.txt.
        echo "${green}[*] Performing passive recon with CLI tools and online databases"
        { 
          subfinder -d $domain -all -silent -recursive | grep "$domain";
          assetfinder -subs-only $domain | grep "$domain";
          curl -s "https://crt.sh/?q=%25.$domain&output=json" | jq -r '.[].name_value' | sed 's/\*\.//g' | grep "$domain";
          curl -s "http://web.archive.org/cdx/search/cdx?url=*.$domain/*&output=text&fl=original&collapse=urlkey" | sed -e 's_https*://__' -e "s/\/.*//" | grep "$domain";
          curl -s "https://otx.alienvault.com/api/v1/indicators/domain/$domain/url_list?limit=100&page=1" | grep -o '"hostname": *"[^"]*' | sed 's/"hostname": "//' | grep "$domain";
        } | grep -v -E "=" | sort | uniq | sort | uniq > $dirs/subs.txt
        wordcount=$(wc -l < $dirs/subs.txt)
        echo "${green}[-] Found $wordcount subdomains" 
}

filtering_scanning()
{
        # Check what subdomains are up / responsive.
        echo "${green}[*] Filtering out"
        cat $dirs/subs.txt | sort | uniq | grep $domain | httpx -threads 200 -ports 80,443,8000,8080,8443,8888 > $dirs/alive.txt
        cat $dirs/alive.txt | httpx -threads 200 -mc 200 > $dirs/200.txt
        echo "${red}[!] FILTERING COMPLETE"
        echo "${green}[*] Generating screenshots and a HTML report with aquatone"
        cat $dirs/alive.txt | aquatone -out $aquatonedir
}

urlgathering()
{
        echo "${green}[*] Gathering URLs"
        cat $dirs/alive.txt | katana -d 5 -ps -pss waybackarchive,commoncrawl,alienvault -kf -jc -fx -ef woff,css,png,svg,jpg,woff2,jpeg,gif,svg | grep $domain > $urldir/main.txt
        cat $dirs/alive.txt | gau | grep $domain >> $urldir/main.txt
        echo "${green}[*] Extracting endpoints of interest"
        grep -E "\.js$|\.php$|\.asp$|\.cgi$|\.jsp$" $urldir/main.txt | grep $domain > $urldir/endpoints.txt
        grep "\.js$" $urldir/endpoints.txt > $urldir/jsfiles.txt                                     
        grep "\.php$" $urldir/endpoints.txt > $urldir/phpfiles.txt
        grep "\.asp$" $urldir/endpoints.txt > $urldir/aspfiles.txt
        grep "\.cgi$" $urldir/endpoints.txt > $urldir/cgifiles.txt
        grep "\.jsp$" $urldir/endpoints.txt > $urldir/jspfiles.txt
        grep -E "/api|/v[12]|_api" $urldir/main.txt > $urldir/api-endpoints.txt
        grep "=" $urldir/main.txt | grep $domain > $urldir/params.txt
}

main
